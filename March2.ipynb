{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics for March 2, 2016\n",
    "\n",
    "My goal is to provide fast methods to use Markov-Chain Monte Carlo (MCMC) methods on linear mixed-effects models, with possible extensions to generalized linear mixed models.\n",
    "\n",
    "There are several MCMC frameworks for [Julia](http://julialang.org).  An interface to [Stan](http://mc-stan.org) is available and there are two native Julia implementations; [Mamba](https://github.com/brian-j-smith/Mamba.jl) and [Lora](https://github.com/JuliaStats/Lora.jl).  I prefer `Mamba` because of its flexibility.  The problem with Stan, BUGS and JAGS is that each of them reinvents all the data structures, input/output, data manipulation, distribution definitions, etc. in its own environment.  They also define a Domain Specific Language (DSL) for which they must provide parsers, interpreters, run-time environments, etc.\n",
    "\n",
    "A native implementation like Mamba can use all of the facilities of Julia and its packages.\n",
    "\n",
    "## Linear predictor\n",
    "\n",
    "Whenever you have a linear predictor (i.e. an $\\bf X\\beta$ type of expression) in a model there is a good chance that you can write out the full conditional distribution of $\\beta$ or obtain a good approximation to it.  If you can write out the conditional distribution you can use a multivariate Gibbs sampler to obtain a vector-valued sample from the condtional.  This helps to avoid one of the underlying problems of MCMC methods which is successive sampling from conditionals of correlated parameters.  Consider a case where you might have hundreds or thousands of random effects and dozens of fixed effects.  You don't want to sample sequentially in those cases when you can sample from the distribution of the entire vector in one step.\n",
    "\n",
    "## Multivariate normal conditionals\n",
    "\n",
    "In most cases the conditional distribution of the coefficients (i.e. both random and fixed effects) is a multivariate normal with known mean and covariance matrix.  It is worthwhile examining the representation in Julia of this distribution.  Not surprisingly the representation involved the mean and covariance but the form of the covariance is encoded in the type.  For example, a common prior distribution for coefficients is a zero-mean multivariate normal with a covariance that is a large multiple of the identity - a diffuse prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZeroMeanIsoNormal(\n",
       "dim: 2\n",
       "μ: [0.0,0.0]\n",
       "Σ: 2x2 Array{Float64,2}:\n",
       " 1.0e6  0.0  \n",
       " 0.0    1.0e6\n",
       ")\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributions, Mamba, PDMats\n",
    "d = MvNormal(2, 1000.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take apart the representation itself you discover that the only values that are stored are the scalar $\\sigma^2$ and its inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Symbol,1}:\n",
       " :μ\n",
       " :Σ"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldnames(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Distributions.ZeroVector{Float64}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(d.μ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDMats.ScalMat{Float64}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(d.Σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Symbol,1}:\n",
       " :dim      \n",
       " :value    \n",
       " :inv_value"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldnames(d.Σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,1.0e6,1.0e-6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d.Σ.dim, d.Σ.value, d.Σ.inv_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with a prior\n",
    "\n",
    "Let's defer the issue of a prior for a moment and consider that we have the matrix $\\bf X'X$, the vector $\\bf X'y$ and the scalar $\\sigma$ defining the log-likelihood.  Without the prior, the mean is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5x2 Array{Float64,2}:\n",
       " 1.0  1.0\n",
       " 1.0  2.0\n",
       " 1.0  3.0\n",
       " 1.0  4.0\n",
       " 1.0  5.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = hcat(ones(5), [1.:5;])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base.LinAlg.Cholesky{Float64,Array{Float64,2}} with factor:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PDMats.PDMat{Float64,Array{Float64,2}}(2,2x2 Array{Float64,2}:\n",
       "  5.0  15.0\n",
       " 15.0  55.0,2x2 UpperTriangular{Float64,Array{Float64,2}}:\n",
       " 2.23607  6.7082 \n",
       " 0.0      3.16228)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XtX = PDMat(X'X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 15.0\n",
       " 53.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1.,3,3,3,5];\n",
    "Xty = X'y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6\n",
       " 0.8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "βhat= XtX\\Xty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
