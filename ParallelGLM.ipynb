{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:6598deb9793ef38a663813d970a953b07f853fa0343898c819da697e014ee794"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Fitting GLMs with parallel computation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Methods for fitting [generalized linear models](http://en.wikipedia.org/wiki/Generalized_linear_model) (GLMs) provide an illustration of combining parallelization with devectorization in [Julia](http://www.julialang.org).\n",
      "\n",
      "A GLM models an $n$-dimensional response vector, $\\bf y$, via a [linear predictor](http://en.wikipedia.org/wiki/Linear_predictor_function), $\\bf X\\beta$, that incorporates a $p$-dimensional coefficient parameter vector, $\\beta$, and an $n\\times p$ _model matrix_, $\\bf X$, which is derived from observed covariate values and the form of the model.  The specification of the model includes the distribution family, $\\mathbb{D}$, and the link, $\\mathbb{L}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Specification of the link and the distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We model the response as the realization of a $n$-dimensional random variable, $\\mathcal{Y}$, in which the individual components are independent with means $\\mu_i,i=1,\\dots,n$ and possibly a common scale parameter, $\\phi$.  The $i$th component of the linear predictor, $\\bf\\eta=X\\beta$, is related to the mean, $\\mu_i$ through the scalar link function, $g$.  That is\n",
      "$$g(\\mu_i)=\\eta_i,\\quad i=1,\\dots,n.$$\n",
      "\n",
      "For most GLM specifications we can consider the link function as mapping the allowed range of values of $\\mu_i$ to the entire real line, $\\mathbb{R}$.  The scalar link should be continuous, differentiable and invertible.  Because such a function must be monotone, we can, without loss of generality, assume that it is monotone increasing over its domain.\n",
      "\n",
      "For distributions in the [exponential family](http://en.wikipedia.org/wiki/Exponential_family) there is a canonical parameter which produces a _canonical link_.  Examples include\n",
      "- the __normal__ (also called the __Gaussian__) distribution for which the canonical link is the __identity link__,\n",
      "$$\\eta=g(\\mu)=\\mu,\\quad-\\infty<\\mu<\\infty.$$\n",
      "- the __Bernoulli__ distribution for binary responses for which the canonical link is the __logit link__, or log-odds ratio\n",
      "$$\\eta=g(\\mu)=\\log\\left(\\frac{\\mu}{1-\\mu}\\right),\\quad0<\\mu<1.$$\n",
      "- the __Poisson__ distribution for which the canconical link is the __log link__,\n",
      "$$\\eta=g(\\mu)=\\log(\\mu),\\quad0<\\mu<\\infty.$$\n",
      "\n",
      "The __binomial__ distribution for the number of \"successes\" (i.e. 1's in a set of 0/1 responses) in $k$ independent, identical Bernoulli trials is converted to the proportion of successes so that $0\\le y\\le 1$ and $0<\\mu<1$, as for the Bernoulli.  The number of trials, $k$, is incorporated as the \"prior weight\" on each of these responses.\n",
      "\n",
      "We represent distributions and links as types.  The distribution types are declared in the `Distributions` package.  For a link type we define methods for the generic functions, `link`, `linkinv` and `d\u03bcd\u03b7`, the derivative of the inverse link."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Distributions\n",
      "abstract Link\n",
      "\n",
      "type IdentityLink <: Link end\n",
      "type InverseLink  <: Link end\n",
      "type LogitLink <: Link end\n",
      "type LogLink <: Link end\n",
      "\n",
      "link(::IdentityLink,\u03bc::FloatingPoint) = \u03bc\n",
      "linkinv(::IdentityLink,\u03b7::FloatingPoint) = \u03b7\n",
      "d\u03bcd\u03b7(::IdentityLink,\u03bc::FloatingPoint) = one(\u03bc)\n",
      "\n",
      "link(::InverseLink,\u03bc::FloatingPoint) = inv(\u03bc)\n",
      "linkinv(::InverseLink,\u03b7::FloatingPoint) = inv(\u03b7)\n",
      "d\u03bcd\u03b7(::InverseLink,\u03b7::FloatingPoint) = -inv(abs2(\u03b7))\n",
      "\n",
      "link(::LogitLink,\u03bc::FloatingPoint) = log(\u03bc/(one(\u03bc)-\u03bc))\n",
      "linkinv(::LogitLink,\u03b7::FloatingPoint) = inv(one(\u03b7) + exp(-\u03b7))\n",
      "d\u03bcd\u03b7(::LogitLink,\u03b7::FloatingPoint) = (e = exp(-abs(\u03b7)); e/abs2(one(e)+e))\n",
      "\n",
      "link(::LogLink,\u03bc::FloatingPoint) = log(\u03bc)\n",
      "linkinv(::LogLink,\u03b7::FloatingPoint) = exp(\u03b7)\n",
      "d\u03bcd\u03b7(::LogLin},\u03b7::FloatingPoint) = exp(\u03b7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LoadError",
       "evalue": "syntax: unexpected \"}\" in argument list\nwhile loading In[1], in expression starting on line 23",
       "output_type": "pyerr",
       "traceback": [
        "syntax: unexpected \"}\" in argument list\nwhile loading In[1], in expression starting on line 23",
        ""
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a `Distribution` type we define an expression for the variance, up to the scale factor when present, as a method for the `Base.var` generic.  This expression is multiplied by a prior weight for each response, if present."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Base.var(::Bernoulli,\u03bc::FloatingPoint) = \u03bc * (one(\u03bc) - \u03bc)\n",
      "Base.var(::Binomial,\u03bc::FloatingPoint) =  \u03bc * (one(\u03bc) - \u03bc)\n",
      "Base.var(::Normal,\u03bc::FloatingPoint) = one(\u03bc)\n",
      "Base.var(::Poisson,\u03bc::FloatingPoint) = \u03bc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "var (generic function with 64 methods)"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Note: These methods are all written for general floating point types. Almost all the time they will be used with `Float64`, also called `Cdouble`, values.  However it is possible that `Float32`, also called `Cfloat`, values may be used if speed is more important than accuracy, or that `BigFloat` values could be used when extra precision is desired.  To avoid mixing types within expressions we use function calls like `one(\u03bc)`, which evaluate to the value `1` of the same type as `\u03bc`.  Naturally, these calls are inlined and there is no extra cost incurred.)\n",
      "\n",
      "We also specify how the deviance is to be evaluated for each distribution.  The deviance is negative twice the log-likelihood evaluated as a function of the mean, and hence of the coefficient vector, $\\beta$, only.  If the distribution has a scale parameter, it is fixed at unity.  Because the components are independent, the log-likelihood is the sum of individual contributions.  By analogy with the case of the normal or Gaussian distribution for which the deviance is\n",
      "$$d(\\beta|\\bf y)=\\|y-X\\beta\\|^2=\\|y-\\mu\\|^2=\\rm\\sum_{i=1}^n(y_i-\\mu_i)^2$$\n",
      "each observation's contribution to the deviance is the __squared deviance residual__."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xlogy{T<:FloatingPoint}(x::T, y::T) = x > zero(T) ? x * log(y) : zero(x)\n",
      "two{T<:FloatingPoint}(x::T) = one(x) + one(x)\n",
      "\n",
      "function devresid2{T<:FloatingPoint}(::Bernoulli,y::T,\u03bc::T,wt::T)\n",
      "    omy = one(T) - y\n",
      "    two(y)*wt*(xlogy(y,y/\u03bc) + xlogy(omy,omy/(one(T)-\u03bc)))\n",
      "end\n",
      "devresid2{T<:FloatingPoint}(::Binomial,y::T,\u03bc::T,wt::T) = devresid2(Bernoulli,y,\u03bc,wt)\n",
      "devresid2{T<:FloatingPoint}(::Normal,y::T,\u03bc::T,wt::T) = wt * abs2(y-\u03bc)\n",
      "devresid2{T<:FloatingPoint}(::Poisson,y::T,\u03bc::T,wt::T) = two(y)*wt*(xlogy(y,y/\u03bc) - (y-\u03bc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "devresid2 (generic function with 4 methods)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function `xlogy` is a generalization of `xlogx`, which evaluates to $x\\log(y)$ with the correct limit at $x=0$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xlogx(x::FloatingPoint) = x > zero(x) ? x * log(x) : zero(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "xlogx (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is common the squared deviance residual to include terms of the form $y\\log(y/\\mu)$ where $y$ can be zero.\n",
      "\n",
      "The final piece is a method to obtain initial estimates in an iterative algorithm.  It is easiest to define initial values for `\u03bc` from which initial values for `\u03b7` can be derived by applying the link function.  This may seem trivial in that we could simply set $\\mu_i=y_i$ except that the values of $y_i$ may not be legitimate values for $\\mu$, as is the case for the Bernoulli where $y_i\\in\\left\\{0,1\\right\\}$ but we must have $0<\\mu_i<1$ for both the link and the inverse link to be well-defined."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "canonicallink(::Bernoulli) = LogitLink()\n",
      "canonicallink(::Binomial) = LogitLink()\n",
      "canonicallink(::Gamma) = InverseLink()\n",
      "canonicallink(::Normal) = IdentityLink()\n",
      "canonicallink(::Poisson) = LogLink()\n",
      "\n",
      "mustart{T<:FloatingPoint}(::Bernoulli,y::T,wt::T) = (wt*y + half(y))/(wt + one(y))\n",
      "mustart{T<:FloatingPoint}(::Binomial,y::T,wt::T) = mustart(Bernoulli,y,wt)\n",
      "mustart{T<:FloatingPoint}(::Gamma,y::T,::T) = y\n",
      "mustart{T<:FloatingPoint}(::Normal,y::T,::T) = y\n",
      "mustart{T<:FloatingPoint}(::Poisson,y::T,::T) = y + inv(convert(typeof(y),10.))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "mustart (generic function with 5 methods)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The IRLS algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "McCullagh and Nelder in their book, __Generalized Linear Models (2nd ed.)__ (Chapman and Hall, 1989) call this the iterative weighted least squares algorithm (p. 40).  The alternative name [\"iteratively reweighted least squares (IRLS)\"](http://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares) is more widely used.\n",
      "\n",
      "As described by McCullagh and Nelder the algorithm determines the maximum likelihood estimates of $\\beta$.  The $i+1$st coefficient vector, $\\beta^{(i+1)}$, is obtained from the $i$th coefficient vector, $\\beta^{(i)}$ by solving a weighted least squares problem of the form\n",
      "$$\\bf X'W^{(i)}X \\beta^{(i+1)}=X'Wz^{(i)}$$\n",
      "where the $n\\times n$ diagonal matrix $\\bf W^{(i)}$ represents the _working weights_ and $\\bf z^{(i)}$ isthe $n$-dimensional _working response vector_, which McCullagh and Nelder call the \"adjusted dependent covariate\".\n",
      "\n",
      "All the calculations in the iteration, up to the actual solution of the normal equations, can be performed elementwise.  The expressions for W and z are given in the function `XtXXXtWz!` which evaluates the products $\\bf X'W^{(i)}X$ and $\\bf X'W^{(i)}z^{(i)}$ in pre-allocated storage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function XtWXXtWz!{T<:FloatingPoint}(XtWX::Matrix{T},XtWz::Vector{T},\n",
      "    Xt::Matrix{T},\u03b2::Vector{T},y::Vector{T},wt::Vector{T},D::Distribution,L::Link)\n",
      "                     # check arguments\n",
      "    p,n = size(Xt); r,s = size(XtWX)\n",
      "    p == r == s == length(\u03b2) == length(XtWz) || throw(DimensionMismatch(\"\"))\n",
      "    n == length(y) == length(wt) || throw(DimensionMismatch(\"\"))\n",
      "                     # initialize output arrays and deviance\n",
      "    fill!(XtWX,zero(T))\n",
      "    fill!(XtWz,zero(T))\n",
      "    dev = zero(T)\n",
      "    \n",
      "    @inbounds for i in 1:n\n",
      "        \u03b7 = zero(T)\n",
      "        for j in 1:p\n",
      "            \u03b7 += Xt[j,i] * \u03b2[j]\n",
      "        end\n",
      "        \u03bc = linkinv(L,\u03b7)\n",
      "        dev += devresid2(D,y[i],\u03bc,wt[i])\n",
      "        \u03bc\u03b7 = d\u03bcd\u03b7(L,\u03b7)\n",
      "        W = wt[i] * abs2(\u03bc\u03b7) / var(D,\u03bc)\n",
      "        z = \u03b7 + (y[i] - \u03bc)/\u03bc\u03b7\n",
      "        for j in 1:p\n",
      "            for ii in j:p  # lower triangle of XtWX\n",
      "                XtWX[ii,j] += W*Xt[j,i]*Xt[ii,i]\n",
      "            end\n",
      "            XtWz[j] += W*Xt[j,i]*z\n",
      "        end\n",
      "    end\n",
      "    dev\n",
      "end\n",
      "\n",
      "n = 1_000_000;\n",
      "p = 20\n",
      "srand(1234321)\n",
      "                   # simulated data\n",
      "const Xt = hcat(ones(n),randn(n,p-1))';\n",
      "const \u03b2true = rand(p);\n",
      "const y = [rand() < \u03b7 ? 1. : 0. for \u03b7 in Xt'*\u03b2true];\n",
      "const wt = ones(n);\n",
      "\n",
      "const \u03b2 = zeros(p);\n",
      "const XtWX = zeros(p,p);\n",
      "const XtWz = zeros(p);\n",
      "const D = Bernoulli();\n",
      "const L = LogitLink();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "XtWXXtWz!(XtWX,XtWz,Xt,\u03b2,y,wt,D,L)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "1.386294361132449e6"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "XtWX"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "20x20 Array{Float64,2}:\n",
        " 250000.0           0.0          0.0     \u2026       0.0         0.0         0.0\n",
        "     50.4437   249740.0          0.0             0.0         0.0         0.0\n",
        "   -133.807       356.313   249816.0             0.0         0.0         0.0\n",
        "   -120.314        59.8364     595.339           0.0         0.0         0.0\n",
        "   -313.578      -298.327     -131.865           0.0         0.0         0.0\n",
        "   -587.486       213.643      198.938   \u2026       0.0         0.0         0.0\n",
        "      1.95856    -346.497       38.0878          0.0         0.0         0.0\n",
        "    447.917      -100.126      -39.0773          0.0         0.0         0.0\n",
        "    347.69       -132.85        78.7224          0.0         0.0         0.0\n",
        "    601.111       180.009      389.847           0.0         0.0         0.0\n",
        "     78.868      -118.195     -310.491   \u2026       0.0         0.0         0.0\n",
        "   -169.818        89.9519    -435.605           0.0         0.0         0.0\n",
        "    -36.4061     -299.145      193.808           0.0         0.0         0.0\n",
        "     37.2612     -139.923     -130.512           0.0         0.0         0.0\n",
        "    123.901      -172.575      153.344           0.0         0.0         0.0\n",
        "    -26.805       305.471     -168.175   \u2026       0.0         0.0         0.0\n",
        "    134.817      -356.065      287.815           0.0         0.0         0.0\n",
        "    -36.4425      -90.3045     122.105      249931.0         0.0         0.0\n",
        "    193.901        17.4012    -220.525        -400.252  250444.0         0.0\n",
        "    110.503        87.5067    -170.632         189.633     -70.004  250505.0"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@time XtWXXtWz!(XtWX,XtWz,Xt,\u03b2,y,wt,D,L);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "elapsed time: 0."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "597245611 seconds (13896 bytes allocated)\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "XtWz'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "1x20 Array{Float64,2}:\n",
        " -58598.0  41342.3  58840.7  168689.0  \u2026  43760.3  120636.0  42883.5  41171.7"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two versions of the IRLS algorithm.  In the _fixed-point_ version we solve for a new parameter vector $\\beta^{(i+1)}$ at the $i$th iteration.  In the _increment_ version we solve for the parameter increment, $\\delta_\\beta^{(i)}$ at the $i$th iteration.\n",
      "\n",
      "In the increment form, the $i$th _working residual_ is\n",
      "$$r_i=\\frac{y_i-\\mu_i}{d\\mu_i/d\\eta_i}$$\n",
      "and the $i$th _working weight_ is\n",
      "$$W_i=\\frac{w_i}{v_i}*\\left(d\\mu_i/d\\eta_i\\right)^2$$\n",
      "where $w_i$ is the $i$th prior weight and $v_i$ is the scalar variance, as evaluated by the appropriate `Base.var` method.\n",
      "\n",
      "In the fixed-point form we define a _working response_ as\n",
      "$$z_i=r_i+\\eta_i.$$\n",
      "\n",
      "Because we start with values of $\\mu$ and $\\eta=\\bf g(\\mu)$, we must use a fixed-point iteration to create $\\beta^1$.  After that I prefer to use the increment iterative form.\n",
      "\n",
      "Because Julia uses column-major ordering in arrays and we wish to distribute the calculations for blocks of observations, we store the transpose, $\\bf X'$ of the model matrix.  For each observation we update local accumulators for $\\bf X'WX$ and $\\bf X'Wz$.\n",
      "\n",
      "By turning off bounds checking in the main loop of this function we save about 1/3 of the execution time. \n",
      "\n",
      "Because there is little overhead in evaluating the squared deviance residuals we return the deviance at $\\beta^{(i)}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Solving for $\\beta^{(i+1)}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A_ldiv_B!(cholfact!(Base.LinAlg.copytri!(XtWX,'L')), copy!(\u03b2,XtWz))'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "1x20 Array{Float64,2}:\n",
        " -0.235627  0.16705  0.233443  0.672734  \u2026  0.481905  0.169487  0.165014"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "XtWXXtWz!(XtWX,XtWz,Xt,\u03b2,y,wt,D,L)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "640285.7509753274"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repeating this process several times gets us close to the mle's but then runs into trouble."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in 1:6\n",
      "    A_ldiv_B!(cholfact!(Base.LinAlg.copytri!(XtWX,'L')), copy!(\u03b2,XtWz))\n",
      "    println(XtWXXtWz!(XtWX,XtWz,Xt,\u03b2,y,wt,D,L))\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "410704."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "22804175573\n",
        "286704"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".0429033362\n",
        "219694"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".2235221154\n",
        "188193"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".71310208837\n",
        "NaN"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "LoadError",
       "evalue": "PosDefException(1)\nwhile loading In[13], in expression starting on line 1",
       "output_type": "pyerr",
       "traceback": [
        "PosDefException(1)\nwhile loading In[13], in expression starting on line 1",
        "",
        " in cholfact! at linalg/factorization.jl:36",
        " in cholfact! at linalg/factorization.jl:30",
        " in anonymous at no file:2"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we traced back the origin of the `NaN`'s we would find that they originate in the evaluation of $\\mu$ for large values of $\\eta$.  If $e^{-\\eta}$ is less than the relative machine precision, then $\\mu=1/(1+e^{-\\eta})$ evaluates to 1.0 and the variance, $\\mu(1-\\mu)$ evaluates to zero.  Dividing by the variance in the expression for $W$ produces the `NaN`'s.  The simplest fix for this is to put a lower bound on the variance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function XtWXXtWz!{T<:FloatingPoint}(XtWX::Matrix{T},XtWz::Vector{T},\n",
      "    Xt::Matrix{T},\u03b2::Vector{T},y::Vector{T},wt::Vector{T},D::Distribution,L::Link)\n",
      "                     # check arguments\n",
      "    p,n = size(Xt); r,s = size(XtWX)\n",
      "    p == r == s == length(\u03b2) == length(XtWz) || throw(DimensionMismatch(\"\"))\n",
      "    n == length(y) == length(wt) || throw(DimensionMismatch(\"\"))\n",
      "                     # initialize output arrays and deviance\n",
      "    fill!(XtWX,zero(T))\n",
      "    fill!(XtWz,zero(T))\n",
      "    dev = zero(T)\n",
      "    \n",
      "    @inbounds for i in 1:n\n",
      "        \u03b7 = zero(T)\n",
      "        for j in 1:p\n",
      "            \u03b7 += Xt[j,i] * \u03b2[j]\n",
      "        end\n",
      "        \u03bc = linkinv(L,\u03b7)\n",
      "        dev += devresid2(D,y[i],\u03bc,wt[i])\n",
      "        \u03bc\u03b7 = d\u03bcd\u03b7(L,\u03b7)\n",
      "        W = wt[i] * abs2(\u03bc\u03b7) / max(eps(T),var(D,\u03bc))\n",
      "        z = \u03b7 + (y[i] - \u03bc)/\u03bc\u03b7\n",
      "        for j in 1:p\n",
      "            for ii in j:p  # lower triangle of XtWX\n",
      "                XtWX[ii,j] += W*Xt[j,i]*Xt[ii,i]\n",
      "            end\n",
      "            XtWz[j] += W*Xt[j,i]*z\n",
      "        end\n",
      "    end\n",
      "    dev\n",
      "end\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "XtWXXtWz! (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fill!(\u03b2,0.)\n",
      "for i in 1:10\n",
      "    println(XtWXXtWz!(XtWX,XtWz,Xt,\u03b2,y,wt,D,L))\n",
      "    A_ldiv_B!(cholfact!(Base.LinAlg.copytri!(XtWX,'L')), copy!(\u03b2,XtWz))\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "386294361132449e6\n",
        "640285"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".7509753274\n",
        "410704"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".22804175573\n",
        "286704"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".0429033362\n",
        "219694"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".2235221154\n",
        "188193"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".71310208837\n",
        "177918"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".40615398\n",
        "176412"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".76442151\n",
        "176369"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".65645628446\n",
        "176369"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".61259003764\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parallelizing the calculations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have structured the IRLS calculations so that the they can be parallelized relatively easily using `SharedArray`s in Julia with multiple processes on a single computer or, even better, using threads, when they become available.\n",
      "\n",
      "The important characteristic of a SharedArray object is that it has a process id's over which it is shared.  In other aspects it behaves like a non-shared array.  We can parallelize on multiple shared arrays provided that the set of processes is consistent.\n",
      "\n",
      "In a Julia type for GLM's we will use `DenseArray`, which is an abstract type containing shared arrays and single-process arrays, as the type for different members."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type ParallelGLM{T<:FloatingPoint}\n",
      "    Xt::DenseMatrix{T}  # transpose of the model matrix\n",
      "    y::DenseVector{T}   # response\n",
      "    wt::DenseVector{T}  # prior weights\n",
      "    D::Distribution\n",
      "    L::Link\n",
      "    \u03b2::DenseVector{T}   # coefficients\n",
      "    XtWX::Array{T,3}\n",
      "    XtWz::Matrix{T}\n",
      "end\n",
      "\n",
      "function ParallelGLM{T<:FloatingPoint}(Xt::DenseMatrix{T},y::DenseVector{T},\n",
      "    wt::DenseVector{T},D::Distribution,L::Link)\n",
      "    p,n = size(Xt)\n",
      "    n = length(y) = length(wt) || throw(DimensionMismatch(\"\"))\n",
      "    if isa(Xt,SharedArray)\n",
      "        if !isa(y,SharedArray)\n",
      "            yy = similar(Xt, (n,))\n",
      "            copy!(sdata(yy), y)\n",
      "            y = yy\n",
      "        end\n",
      "        if !isa(wt,SharedArray)\n",
      "            ww = similar(Xt, (n,))\n",
      "            copy!(sdata(ww),wt)\n",
      "            wt = ww\n",
      "        end\n",
      "        Set(procs(Xt)) == Set(procs(y)) == Set(procs(wt)) || \n",
      "           error(\"procs of shared arrays must be identical\")\n",
      "    end \n",
      "    ParallelGLM(Xt,y,wt,D,L,similar(Xt,(p,))))\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LoadError",
       "evalue": "syntax: extra token \")\" after end of expression\nwhile loading In[16], in expression starting on line 30",
       "output_type": "pyerr",
       "traceback": [
        "syntax: extra token \")\" after end of expression\nwhile loading In[16], in expression starting on line 30",
        ""
       ]
      }
     ],
     "prompt_number": 16
    }
   ],
   "metadata": {}
  }
 ]
}